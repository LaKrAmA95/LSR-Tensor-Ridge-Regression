{"cells":[{"cell_type":"markdown","metadata":{"id":"KzKHJMnbfVlm"},"source":["# Tensor Pipeline\n","\n","This note book runs the  lsr structured tensor ridge regression model with the following parameters fixed.\n","\n","1. Ridge regression coefficient: \n","2. Max iterations\n","3. Separation Rank\n","4. Tucker Rank\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uGRufj_xfVlo"},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12586,"status":"ok","timestamp":1711494542524,"user":{"displayName":"Lakshitha Ramanayake Mudiyanselage","userId":"09058794058790427589"},"user_tz":240},"id":"Z5jGDIH5fVlo","outputId":"c31a3910-211e-4265-d209-a5cda34ef592"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: dill in /Users/lakrama/miniconda3/envs/torchtensor/lib/python3.12/site-packages (0.3.8)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install dill"]},{"cell_type":"markdown","metadata":{"id":"7KNNEWuzfVlp"},"source":["## System Path"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"v1121WdGM3gG"},"outputs":[],"source":["import sys\n","import platform\n","\n","# Check the operating system\n","if platform.system() == \"Windows\":\n","    # Using double backslashes\n","    sys.path.append(r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Closed_Form_Solver\\Code Files\")\n","elif platform.system() == \"Darwin\":  # macOS\n","    # Append path for macOS\n","     sys.path.append(\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Closed_Form_Solver/Code Files\")\n","     sys.path.append('/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Data_Sets/HCP')"]},{"cell_type":"markdown","metadata":{"id":"BMZmvyrBfVlq"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"tH57r_wdfVlq"},"outputs":[],"source":["#Import sklearn stuff\n","import datetime\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import re\n","import scipy\n","from sklearn.metrics import r2_score \n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns\n","\n","#Used to load data from pkl file\n","import dill\n","\n","#Import External Files\n","from KFoldCV import KFoldCV\n","from train_test import train_test\n","from DataGenerationB import *"]},{"cell_type":"markdown","metadata":{},"source":["## Functions we  need"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#conversion of the HCP data from Vector --> Matrix\n","\n","def vectomat_matlab(vector, outputdim):\n","    '''\n","    This code is to regenerate the symmetric functional connectivity matrix from the given vectorized upper triangular portion.\n","    This code accounts for the mismatch between MATLAB and Python indexing.\n","\n","    vector: ndarray - the sample vector as a column vector (:,1)\n","    outputdim: scalar - the dimension of the symmetric matrix\n","    '''\n","\n","    # Checking whether the vector dimension and the desired output dimensions match\n","    vector_length = vector.shape[0]\n","    desired_length = outputdim * (outputdim - 1) / 2\n","\n","    # Check if lengths match and raise an error if not\n","    if vector_length != desired_length:\n","        raise ValueError(\"Vector length is insufficient to construct the symmetric matrix.\")\n","    \n","    # Create a symmetric matrix with zeros\n","    matrix = np.zeros((outputdim, outputdim))\n","    \n","    p = 0\n","    \n","    for i in range(outputdim):\n","        for j in range (i,outputdim):\n","            if i == j :\n","                matrix[i,j] = 1\n","            else:\n","                matrix[i,j] = vector[p]\n","                matrix[j,i] = matrix[i,j]\n","                p = p+1\n","                \n","    return matrix\n","\n","\n","def samplestomat(dataset,outputdim):\n","    \n","    '''\n","    This code is developed to convert the vectorized data matrix in to a 3D data tensor.\n","    \n","    dataset : nd:array - (samples*features)\n","    outputdim : scalar\n","\n","    '''\n","\n","    #number of samples\n","    n_samples = dataset.shape[0]\n","    #3D matrix to hold the output\n","    out_dataset = np.zeros((n_samples,outputdim,outputdim))\n","\n","    for p in range(n_samples):\n","        \n","        sample = dataset[p]\n","        sample = vectomat_matlab(sample,outputdim)\n","        out_dataset[p] = sample\n","\n","    random_index = np.random.randint(0, n_samples)\n","    random_sample = out_dataset[random_index]\n","\n","    # Plot the heatmap\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(random_sample, cmap='viridis', cbar=True)\n","    plt.title(f'Heatmap of Random Sample {random_index}')\n","    plt.show()\n","\n","    return out_dataset\n","\n","\n","def normalize_by_frobenius_norm(samples):\n","    \"\"\"\n","    Normalizes each sample (2D matrix) in the array by its Frobenius norm.\n","\n","    Parameters:\n","    samples (numpy.ndarray): A 3D numpy array with dimensions [samples, rows, columns].\n","\n","    Returns:\n","    numpy.ndarray: A 3D numpy array with each sample normalized by its Frobenius norm.\n","    \"\"\"\n","    # Calculate the Frobenius norm for each sample\n","    frobenius_norms = np.linalg.norm(samples, axis=(1, 2))\n","    \n","    # Reshape the norms to broadcast correctly for division\n","    frobenius_norms = frobenius_norms[:, np.newaxis, np.newaxis]\n","    \n","    # Normalize each sample by its Frobenius norm\n","    normalized_samples = samples / frobenius_norms\n","    \n","    return normalized_samples\n","        \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RraGuUhGfVls"},"source":["## Import Data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import platform\n","import numpy as np\n","\n","if platform.system() == \"Windows\": \n","    file_path = r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\fmri_rs.npy\"\n","elif platform.system() == \"Darwin\":\n","    file_path = r\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Data_Sets/HCP/Resting State FMRI/fmri_rs.npy\"  # Adjust the path for macOS\n","\n","with open(file_path, \"rb\") as f:\n","    fmri_rs = np.load(f)\n","\n","# Each sample is a row\n","fmri_rs = fmri_rs.T\n","\n","\n","# Determine the platform and load the appropriate file\n","if platform.system() == \"Windows\": \n","    mat_file_path = r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\MMP_HCP_60_splits.mat\"\n","elif platform.system() == \"Darwin\":\n","    mat_file_path = \"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Data_Sets/HCP/Resting State FMRI/MMP_HCP_60_splits.mat\"\n","else:\n","    raise ValueError(\"Unsupported platform\")\n","\n","# Load the .mat file\n","mat_file = scipy.io.loadmat(mat_file_path)\n","\n","#Iterating over seeds\n"]},{"cell_type":"markdown","metadata":{},"source":["1. Need Parameters"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#number of seeds we are considering \n","number_seeds = 10\n","\n","#hyper parameters\n","tensor_dimensions = np.array([400,400])\n","tensor_mode_ranks = np.array([4,4])\n","separation_rank = 2\n","\n","#For now, define finite alpha set that we are searching over\n","alphas =  [0.1,0.4,0.7,1,1.5,2,2.5,3,3.5,4,5,10,15,20,25,30]\n","\n","# The dataframe to hold the results for a seed\n","columns = ['Seed', 'Best Lambda', 'NMSE', 'CORR', 'R2', 'Time Taken', 'Gradient']\n","results_df = pd.DataFrame(columns=columns)\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"no field of name seed_0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_seeds):\n\u001b[1;32m      2\u001b[0m         \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Extract subject lists from the loaded file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     seed_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmat_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfolds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseed_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     subject_lists \u001b[38;5;241m=\u001b[39m seed_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_fold\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_list\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     test_subjects \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m subject_lists[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()]\n","\u001b[0;31mValueError\u001b[0m: no field of name seed_0"]}],"source":["\n","for seed in range(number_seeds):\n","        \n","    # Extract subject lists from the loaded file\n","    seed_1 = mat_file['folds'][f'seed_{seed+1}'][0, 0]\n","    subject_lists = seed_1['sub_fold'][0, 0]['subject_list']\n","    test_subjects = [int(item[0]) for item in subject_lists[0, 0].flatten()]\n","\n","    #Getting the HCP test subjects \n","\n","    #array to hold the  subjects\n","    HCP_753_Subjects = []\n","\n","    #setting the file path\n","    if platform.system() == \"Windows\":\n","        file_path = r'D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\MMP_HCP_753_subs.txt'\n","    elif platform.system() == \"Darwin\":\n","        file_path = '/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Data_Sets/HCP/Resting State FMRI/MMP_HCP_753_subs.txt'\n","    #if file pat  h is returned then load\n","    if file_path:\n","        try:\n","            with open(file_path, 'r') as file:\n","                HCP_753_Subjects = [int(line.strip()) for line in file.readlines()]\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","\n","\n","    #Put the HCP test subjects into a dataframe\n","    # Determine the platform and load the appropriate file\n","    if platform.system() == \"Windows\":\n","        csv_file_path = r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\MMP_HCP_componentscores.csv\"\n","    elif platform.system() == \"Darwin\":\n","        csv_file_path = \"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Data_Sets/HCP/Resting State FMRI/MMP_HCP_componentscores.csv\"\n","\n","    df = pd.read_csv(csv_file_path)\n","    df['Subject'] = pd.to_numeric(df['Subject'], errors='coerce')\n","    df = df[df['Subject'].isin(HCP_753_Subjects)].reset_index(drop = True)\n","\n","    #Split all our data into a Train and Test Set\n","    df_train, df_test = df[~df['Subject'].isin(test_subjects)], df[df['Subject'].isin(test_subjects)]\n","\n","    #Create train and test arrays\n","    train_subjects = df_train.index.to_list()\n","    test_subjects = df_test.index.to_list()\n","\n","    #Reshape labels into column vector\n","    X_train_vec, Y_train = fmri_rs[train_subjects], df_train[\"varimax_cog\"].to_numpy().reshape((-1, 1))\n","    X_test_vec, Y_test = fmri_rs[test_subjects], df_test[\"varimax_cog\"].to_numpy().reshape((-1, 1))\n","\n","    #Training data\n","    X_train = samplestomat(X_train_vec,400)\n","    X_test  = samplestomat(X_test_vec,400)\n","    Y_train = Y_train.reshape(-1)\n","    Y_test = Y_test.reshape(-1)\n","\n","\n","    print(X_train.shape)\n","    print(Y_train.shape)\n","    print(X_test.shape)\n","    print(Y_test.shape)\n","\n","    #Function to row wise normalizati\n","\n","    X_train = normalize_by_frobenius_norm(X_train)\n","    X_test = normalize_by_frobenius_norm(X_test)\n","\n","    #number of samples in train and test \n","    n_train = X_train.shape[0]\n","    n_test = X_test.shape[0]\n","\n","    # Reshape the 3D array to a 2D array where each row represents a sample\n","    # The shape of the original 3D array is (n_samples, n_features_per_sample, n_dimensions)\n","    # We reshape it to (n_samples, n_features_per_sample * n_dimensions)\n","\n","    X_train_2D = X_train.reshape(n_train, -1)\n","    X_test_2D = X_test.reshape(n_test,-1)\n","\n","    # Initialize StandardScaler\n","    scaler = StandardScaler(with_std = False) #standard scalar only\n","\n","    # Fit scaler on train data and transform train data\n","    X_train_scaled = scaler.fit_transform(X_train_2D)\n","    # Transform test data using the scaler fitted on train data\n","    X_test_scaled = scaler.transform(X_test_2D)\n","\n","    # Reshape the scaled data back to 3D\n","    X_train = X_train_scaled.reshape(n_train, X_train.shape[1],X_train.shape[2])\n","    X_test  = X_test_scaled.reshape(n_test, X_test.shape[1],X_train.shape[2])\n","\n","    #average response value\n","    Y_train_mean = np.mean(Y_train)\n","    # Mean centering y_train and y_test\n","    Y_train = Y_train - Y_train_mean\n","\n","\n","    print(\"Sample mean for each feature (across samples):\",scaler.mean_)\n","    print(\"Sample variance for each feature (across samples):\",scaler.var_)\n","    print('Response Average:',Y_train_mean)\n","\n","    #Training\n","\n","    #Define Number of Folds we want\n","    k_folds = 5\n","    hypers = {'max_iter': 50, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n","    lambda1, validation_normalized_estimation_error, validation_nmse_losses, validation_correlations, validation_R2_scores, objective_function_information,gradient_information = KFoldCV(X_train, Y_train, alphas, k_folds, hypers, B_tensored = None, intercept= False)\n","\n","    #Tesing \n","\n","    start_time = time.time()\n","    hypers = {'max_iter': 50, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n","    test_normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_level_values,iterate_level_reconstructed_tensor = train_test(X_train, Y_train, X_test, Y_test, lambda1, hypers,Y_train_mean, B_tensored = None, intercept= False)\n","    end_time = time.time()\n","    execution_time = end_time - start_time\n","    print(f'Time for one lambda:{execution_time}')\n","\n","    #saving data of one seed\n","\n","    formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    max_iter = hypers['max_iter']\n","\n","    if platform.system() == \"Windows\":\n","        pkl_file = rf\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Closed_Form_Solver\\Multimodal\\LSR\\Frobenious\\Resting-Language\\Across_Seeds\\HCP_lambdas_{alphas}_seed_{seed}_sep_{separation_rank}_tucker_{tensor_mode_ranks}.pkl\"\n","    elif platform.system() == \"Darwin\":\n","        pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Stochastic LSR TRR/Experimental Results/ExecutionTime_intercept_5_{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n","\n","    #printing error matrices for one seed\n","\n","    print(f'SEED: {seed+1} : lambda1:{lambda1} : TNMSE:{test_nmse_loss} : TCORR:{test_correlation} : TR2: {test_R2_loss} ')\n","\n","    #saving the data for one seed\n","    with open(pkl_file, \"wb\") as file:\n","        dill.dump((X_train,Y_train,X_test,Y_test, lambda1, validation_normalized_estimation_error, validation_nmse_losses, validation_correlations, validation_R2_scores, objective_function_information,gradient_information,test_normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_level_values,factor_core_iteration), file)\n","\n","    #loading the results to the dataframe\n","\n","    best_lambda = lambda1\n","    nmse = test_nmse_loss\n","    corr = test_correlation\n","    r2 = test_R2_loss\n","    time_taken = execution_time\n","    \n","    # Assuming gradient_values is your array\n","    if np.isnan(gradient_values).any():\n","        gradient = gradient_values\n","    else:\n","        gradient = gradient_values[-1,:,:]\n","    \n","    # Append the results to the dataframe\n","    seed_result_df = pd.DataFrame([{\n","        'Seed': seed,\n","        'Best Lambda': best_lambda,\n","        'NMSE': nmse,\n","        'CORR': corr,\n","        'R2': r2,\n","        'Time Taken': time_taken,\n","        'Gradient': gradient\n","    }])\n","    \n","    #concatenating the results\n","    results_df = pd.concat([results_df,seed_result_df], ignore_index = True) \n","\n","    # Define platform-specific file paths\n","    if platform.system() == 'Windows':\n","        file_path = r'D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Closed_Form_Solver\\Multimodal\\LSR\\Frobenious\\Resting-Language\\Across_Seeds\\Seed_Results'\n","    elif platform.system() == 'Darwin':\n","        file_path = 'addpath'  # Replace 'addpath' with the actual path for macOS\n","\n","    # Save the result as a CSV\n","    results_df.to_csv(f'{file_path}/results_with_matrix_{seed}.csv', index=False)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
